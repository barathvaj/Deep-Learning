{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## In this exercise, we will use the layer model which we built last week. Here we will explore different optimizer functions . In the previous section, we implemented the SGD with momentum. It does this by adding a fraction of the update vector of the past time step to the current update vector which accelerates the parameter updates for faster convergence \n",
    "\n",
    "References\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad\n",
    "\n",
    "A adaptive learning rate which individually adapts the learning rates of all model parameters by scaling them inversaly proportional to the square root of sum of all historical value of the gradient. It adapts to lower learning rates(updates) to the frequently occuring features and larger learning rates/updates to the infrequent feature\n",
    "\n",
    "The main benefits about the Adagrad is it doesn't need the manual tuning of learning rate  and it is intially defaulted to 0.01 and leave it as it is.\n",
    "\n",
    "the main  cons of adagrad is accumulation of squared gradients in the denominator and it keeps growing during the training process. This may shrink the learning rate too too small in way that that it can't move forward to gather additional knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, learning_rate = 0.01, eps = 1e-8):\n",
    "        self.learning_rate = learning_rate \n",
    "        #sum of squares of gradient\n",
    "        self.ssg = None\n",
    "        #smoothing term avoids division by zero\n",
    "        self.eps = eps       \n",
    "    \n",
    "    def update(self, w, grad):\n",
    "        if self.ssg is None:\n",
    "            self.ssg = np.zeros(np.shape(w)) \n",
    "            \n",
    "        # Add the square of the gradient of the loss function at w\n",
    "        self.ssg += np.power(grad, 2)\n",
    "        \n",
    "        # Adaptive gradient with higher learning rate for infrequent data  \n",
    "        return (w - (self.learning_rate * grad)) / np.sqrt(self.ssg+ self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RMSProp\n",
    "\n",
    "RMSProp a adaptive learning techniques which address the shrinking learning rate problem imposed by \"AdaGrad\" in a simple way by taking the running average of squared gradients.\n",
    "\n",
    "cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "\n",
    "x += - learning_rate * dx / (np.sqrt(cache) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RmsProp:\n",
    "    def __init__(self,learning_rate=0.001, decay_rate=0.9,eps =1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        #smoothing term avoids division by zero . same like adagrad\n",
    "        self.eps = eps  \n",
    "        \n",
    "        #running average of gradient - cache\n",
    "        self.rag = None\n",
    "        \n",
    "    def update(self, w, grad):\n",
    "        if self.rag is None:\n",
    "            self.rag = np.zeros(np.shape(grad)) \n",
    "            \n",
    "        \n",
    "        self.rag = (self.decay_rate * self.rag) + ((1- self.decay_rate) * np.power(grad, 2))\n",
    "        \n",
    "         # Adaptive gradient with higher learning rate for infrequent data  \n",
    "        return (w - (self.learning_rate * grad)) / np.sqrt(self.rag+ self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam = Adaptive Moment Estimation\n",
    "\n",
    "Adam takes in addition to the moving average of past squared gradients , it also takes the moving average of the past gradients. in short we can call this as RMSProp with momentum.\n",
    "\n",
    "This is refered from http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants\n",
    "\n",
    "We compute the decaying averages of past and past squared gradients mt and vt respectively as follows\n",
    "\n",
    "mt=β1mt−1+(1−β1)gt\n",
    "\n",
    "vt=β2vt−1+(1−β2)gt**2\n",
    "\n",
    "mt and vt are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As mt and vt are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. β1 and β2 are close to 1).\n",
    "\n",
    "They counteract these biases by computing bias-corrected first and second moment estimates:\n",
    "\n",
    "m-cap= mt/1−β1\n",
    "\n",
    "v-cap=vt/1−β2\n",
    "\n",
    "They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule\n",
    "\n",
    "grad_updated = w - ((learning_rate * m-cap) / sqrt(v-cap) + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001,b1=0.9,b2=0.999,eps =1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #decay rates(b1,be) and smoothing term\n",
    "        self.b1, self.b2,self.eps = b1,b2,eps\n",
    "        #to hold moving averages of the gradients\n",
    "        self.mt = None\n",
    "        #to hold moving averages of the squared gradients\n",
    "        self.vt = None\n",
    "        \n",
    "    def update(self, w, grad):\n",
    "        if self.mt is None:\n",
    "            self.mt = np.zeros(np.shape(grad)) \n",
    "        if self.vt is None:\n",
    "            self.vt = np.zeros(np.shape(grad)) \n",
    "            \n",
    "        self.mt = (self.b1 * self.mt) + ((1-self.b1) * grad)\n",
    "        self.vt = (self.b2 * self.vt) + ((1-self.b2) * np.power(grad,2))\n",
    "        \n",
    "        #to overcome these vectors get biased towards zero\n",
    "        m_cap=  self.mt/(1-self.b1)\n",
    "        v_cap= self.vt/(1-self.b2)\n",
    "        \n",
    "        grad_updated = self.learning_rate * m_cap / (np.sqrt(v_cap) + self.eps)\n",
    "        \n",
    "            \n",
    "        return w - grad_updated\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's test our handwritten dataset with these optimization algorithms, I have added the last session code into a python script named \"DeepLearnerBase.py\". Hence using the same  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DeepLearnerBase import Sequential, Dense, Activation, CrossEntropyForSoftMax,relu,softmax\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat(\"data\\handwritten.mat\")\n",
    "print(data['X'].shape)\n",
    "print(data['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y =  data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 400)\n",
      "(1000, 400)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X,y, test_size=0.20, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "loss = CrossEntropyForSoftMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([    \n",
    "    Dense(100),\n",
    "    Activation(relu),    \n",
    "    Dense(10),\n",
    "    Activation(softmax)    \n",
    "],  optimizer, loss, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| Model Summary |\n",
      "+---------------+\n",
      "Input Shape: 400\n",
      "+------------+-------------+--------------+------------+\n",
      "| Layer Name | Input Shape | Output Shape | Shape      |\n",
      "+------------+-------------+--------------+------------+\n",
      "| Dense      | 400         | 100          | (400, 100) |\n",
      "| relu       | 100         | 100          | (100, 100) |\n",
      "| Dense      | 100         | 10           | (100, 10)  |\n",
      "| softmax    | 10          | 10           | (10, 10)   |\n",
      "+------------+-------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (5 of 10000) |                       | Elapsed Time: 0:00:00 ETA:  0:09:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0 Training Loss:2.324754138876231 Validation Loss: 2.2704057128941066 Training Accuracy:0.0925 Validation Accuracy:0.193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1004 of 10000) |##                  | Elapsed Time: 0:00:48 ETA:  0:07:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1000 Training Loss:0.6545246111218269 Validation Loss: 0.3474585389286208 Training Accuracy:1.0 Validation Accuracy:0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% (2004 of 10000) |####                | Elapsed Time: 0:01:37 ETA:  0:06:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 2000 Training Loss:0.8866205573666668 Validation Loss: 0.408305362960412 Training Accuracy:1.0 Validation Accuracy:0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% (3004 of 10000) |######              | Elapsed Time: 0:02:27 ETA:  0:06:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 3000 Training Loss:1.0397182766869877 Validation Loss: 0.45107997656365545 Training Accuracy:1.0 Validation Accuracy:0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% (4005 of 10000) |########            | Elapsed Time: 0:03:18 ETA:  0:05:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 4000 Training Loss:1.162536772729566 Validation Loss: 0.48619939954213465 Training Accuracy:1.0 Validation Accuracy:0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% (5004 of 10000) |##########          | Elapsed Time: 0:04:08 ETA:  0:03:47"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 5000 Training Loss:1.270333699059994 Validation Loss: 0.5178631301244215 Training Accuracy:1.0 Validation Accuracy:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% (6005 of 10000) |############        | Elapsed Time: 0:04:53 ETA:  0:03:03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 6000 Training Loss:1.3693007216574993 Validation Loss: 0.5478026637751537 Training Accuracy:1.0 Validation Accuracy:0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% (7005 of 10000) |##############      | Elapsed Time: 0:05:38 ETA:  0:02:15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 7000 Training Loss:1.462527951222865 Validation Loss: 0.5762268455951637 Training Accuracy:1.0 Validation Accuracy:0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% (8003 of 10000) |################    | Elapsed Time: 0:06:27 ETA:  0:01:33"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 8000 Training Loss:1.5516516580796622 Validation Loss: 0.6040875292600255 Training Accuracy:1.0 Validation Accuracy:0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% (9004 of 10000) |##################  | Elapsed Time: 0:07:14 ETA:  0:00:47"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 9000 Training Loss:1.637452369519704 Validation Loss: 0.6317042878785042 Training Accuracy:1.0 Validation Accuracy:0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10000 of 10000) |###################| Elapsed Time: 0:08:02 Time: 0:08:02\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train,X_valid,y_valid,epochs= 10000,batchsize= 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we also has other optimization algorithm like Nadam, AdaMax etc... Adam appears to be widely used optimization algorithms, but still in some areas the SGD with momentum outperforms Adam (for ex:- in my last project of detecting solar panel from aerial images). Here it gives the similar performances like SGD with momentum with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
