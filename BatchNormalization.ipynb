{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm\n",
    "\n",
    "When we train a deep neural network, the change of scale in weight from first layer to later layer can be drastically different and it can produce a serious implications say for an ideal learning rate each time. Also we need to be caustious about the weight initialization  strategy and using of higher learning rate. We need a mechanism which standardize weights from first layer to deep into the network which helps in making the training faster , forget about weight initialization and  gradient shrinking and explode issue. That's where the BathNorm comes into picture here. \n",
    "\n",
    "In this exercise,we will extend our layer API to support batch normalization\n",
    "\n",
    "References\n",
    "\n",
    "https://gluon.mxnet.io/chapter04_convolutional-neural-networks/cnn-batch-norm-scratch.html\n",
    "\n",
    "https://wiseodd.github.io/techblog/2016/07/04/batchnorm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation\n",
    "Unlike dropout the batch norm take place usually before activation layer instead of after activation layer. The main idea here, we will normalize the output from linear layer(input to batch norm) so that its distribution is Standard Normal (zero mean and one standard deviation).  \n",
    "\n",
    "\n",
    "<img src=\"files/bforward.png\">\n",
    "\n",
    "### Backward Propogation\n",
    "\n",
    "<img src=\"files/bback.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besides that, in the testing process, we want to use the mean and variance of the complete dataset, instead of those of mini batches. In the implementation, we will accumulate the moving/runninnp.g mean and varience for testing as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from DeepLearnerBase import Layer \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BathNorm(Layer):\n",
    "    def __init__(self, momentum = 0.9,eps =1e-8):\n",
    "        self.momentum = momentum\n",
    "        self.moving_mean = None\n",
    "        self.moving_varience = None  \n",
    "        self.eps = eps\n",
    "        \n",
    "    def setup(self, optimizer,loss):\n",
    "        #based on the documentation read\n",
    "        self.gamma = np.ones(self.inputshape)\n",
    "        self.beta = np.zeros(self.inputshape)\n",
    "        \n",
    "        # parameter optimizers\n",
    "        self.gamma_opt  = copy.copy(optimizer)\n",
    "        self.beta_opt = copy.copy(optimizer)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (self.inputshape ,self.outputshape())\n",
    "        \n",
    "    def forward(self, X, training = True): \n",
    "        if(self.moving_mean is None) :\n",
    "            self.moving_mean = np.mean(X,axis=0)\n",
    "            self.moving_varience = np.var(X,axis =0)\n",
    "            \n",
    "        if(training):\n",
    "            mean =  np.mean(X,axis=0)\n",
    "            varience = np.var(X,axis =0)\n",
    "            \n",
    "            self.moving_mean = (self.momentum * self.moving_mean) + ((1-self.momentum) * mean)\n",
    "            self.moving_varience = (self.momentum * self.moving_varience) + ((1-self.momentum) * varience)\n",
    "        else:\n",
    "            #in the testing process, we want to use the mean and variance of the complete dataset\n",
    "            mean = self.moving_mean \n",
    "            varience = self.moving_varience\n",
    "            \n",
    "        #storing it for backward pass\n",
    "        self.X_centered = X- mean\n",
    "        self.std_dev = 1/(np.sqrt(varience+ self.eps))\n",
    "        \n",
    "        X_norm = self.X_centered * self.std_dev\n",
    "        \n",
    "        #scale and shift\n",
    "        out =  (self.gamma * X_norm) + self.beta\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # Save parameters used during the forward pass\n",
    "        gamma = self.gamma\n",
    "       \n",
    "       \n",
    "        X_norm = self.X_centered * self.std_dev\n",
    "        grad_gamma = np.sum(grad * X_norm, axis=0)\n",
    "        grad_beta = np.sum(grad, axis=0)\n",
    "\n",
    "        self.gamma = self.gamma_opt.update(self.gamma, grad_gamma)\n",
    "        self.beta = self.beta_opt.update(self.beta, grad_beta)\n",
    "\n",
    "        batch_size = grad.shape[0]\n",
    "\n",
    "        # The gradient of the loss with respect to the layer inputs (use weights and statistics from forward pass)\n",
    "        accum_grad = (1 / batch_size) * gamma * self.std_dev * (\n",
    "            batch_size * grad\n",
    "            - np.sum(grad, axis=0)\n",
    "            - self.X_centered * self.std_dev**2 * np.sum(grad * self.X_centered, axis=0)\n",
    "            )\n",
    "\n",
    "        return accum_grad\n",
    "    \n",
    "    def outputshape(self):\n",
    "        return self.inputshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DeepLearnerBase import Sequential, Dense, Activation, CrossEntropyForSoftMax,relu,softmax,SGD\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat(\"data\\handwritten.mat\")\n",
    "print(data['X'].shape)\n",
    "print(data['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y =  data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 400)\n",
      "(1000, 400)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X,y, test_size=0.20, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(learning_rate =  0.1,momentum=0.9)\n",
    "loss = CrossEntropyForSoftMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([    \n",
    "    Dense(100),\n",
    "    BathNorm(),\n",
    "    Activation(relu),    \n",
    "    Dense(50),\n",
    "    BathNorm(),\n",
    "    Activation(relu),    \n",
    "    Dense(10),\n",
    "    BathNorm(),\n",
    "    Activation(softmax)    \n",
    "],  optimizer, loss, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| Model Summary |\n",
      "+---------------+\n",
      "Input Shape: 400\n",
      "+------------+-------------+--------------+------------+\n",
      "| Layer Name | Input Shape | Output Shape | Shape      |\n",
      "+------------+-------------+--------------+------------+\n",
      "| Dense      | 400         | 100          | (400, 100) |\n",
      "| BathNorm   | 100         | 100          | (100, 100) |\n",
      "| relu       | 100         | 100          | (100, 100) |\n",
      "| Dense      | 100         | 50           | (100, 50)  |\n",
      "| BathNorm   | 50          | 50           | (50, 50)   |\n",
      "| relu       | 50          | 50           | (50, 50)   |\n",
      "| Dense      | 50          | 10           | (50, 10)   |\n",
      "| BathNorm   | 10          | 10           | (10, 10)   |\n",
      "| softmax    | 10          | 10           | (10, 10)   |\n",
      "+------------+-------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (2 of 10000) |                       | Elapsed Time: 0:00:00 ETA:  0:23:37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0 Training Loss:1.0312971001220088 Validation Loss: 0.7682038774176346 Training Accuracy:0.788 Validation Accuracy:0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1001 of 10000) |##                  | Elapsed Time: 0:01:57 ETA:  0:18:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1000 Training Loss:0.029824808597426655 Validation Loss: 0.2026289694657334 Training Accuracy:1.0 Validation Accuracy:0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% (2002 of 10000) |####                | Elapsed Time: 0:03:56 ETA:  0:17:46"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 2000 Training Loss:0.02845456320401412 Validation Loss: 0.20786208128671693 Training Accuracy:1.0 Validation Accuracy:0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% (3003 of 10000) |######              | Elapsed Time: 0:05:48 ETA:  0:13:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 3000 Training Loss:0.02800745170782575 Validation Loss: 0.21127903376849794 Training Accuracy:1.0 Validation Accuracy:0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% (4002 of 10000) |########            | Elapsed Time: 0:07:48 ETA:  0:12:53"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 4000 Training Loss:0.02778633210976665 Validation Loss: 0.21454089334569365 Training Accuracy:1.0 Validation Accuracy:0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% (5002 of 10000) |##########          | Elapsed Time: 0:10:49 ETA:  0:12:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 5000 Training Loss:0.027654664660758315 Validation Loss: 0.21704573942161415 Training Accuracy:1.0 Validation Accuracy:0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% (6002 of 10000) |############        | Elapsed Time: 0:12:44 ETA:  0:10:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 6000 Training Loss:0.02756729342278871 Validation Loss: 0.21931392291169435 Training Accuracy:1.0 Validation Accuracy:0.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% (7002 of 10000) |##############      | Elapsed Time: 0:15:01 ETA:  0:05:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 7000 Training Loss:0.027505142916145732 Validation Loss: 0.22131256619582254 Training Accuracy:1.0 Validation Accuracy:0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% (8002 of 10000) |################    | Elapsed Time: 0:17:19 ETA:  0:06:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 8000 Training Loss:0.02745863445597696 Validation Loss: 0.22331078274313718 Training Accuracy:1.0 Validation Accuracy:0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% (9002 of 10000) |##################  | Elapsed Time: 0:19:22 ETA:  0:02:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 9000 Training Loss:0.027422554950695413 Validation Loss: 0.2252454186162289 Training Accuracy:1.0 Validation Accuracy:0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10000 of 10000) |###################| Elapsed Time: 0:21:33 Time: 0:21:33\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train,X_valid,y_valid,epochs= 10000,batchsize= 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you see the above result with BatchNorm, the optimization just zip through and converging in much faster speed (noticed in epoch 2000) when compare to the earlier model even with Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
