{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On ML from scratch exercise, we see how to build a sequential and layer model. Here our focus is to refactor it to better extendability with clean seperation of concern which  makes it very easy to write simple networks that consist of layers layered on top of each other . \n",
    "\n",
    "#### Later after building the layer and sequential model , let discuss and implement the SGD and later in course will discuss about various optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import ipdb as pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from terminaltables import AsciiTable\n",
    "\n",
    "class Sequential:\n",
    "    \"\"\" Accepts \n",
    "    layers - the set of layer layered on top of each other\n",
    "    optimizer - A function which gives the estimate of the gradient which need to be updated to weights to minimize cost functions\n",
    "    loss - the function which evalutes the performance of the model. How the predicted outcome is different from actual outcome. \n",
    "           in short we can call this as error rate\"\"\"\n",
    "    def __init__(self, layers, optimizer, loss, nfeatures):\n",
    "        self.layers, self.optimizer, self.loss = layers, optimizer, loss        \n",
    "        self.progressbar = progressbar.ProgressBar()\n",
    "            \n",
    "        for i in range(0,len(self.layers)):              \n",
    "            self.layers[i].set_inputshape(nfeatures if i == 0 else int(self.layers[i-1].outputshape()))\n",
    "            \n",
    "            #if setup method is there then call setup \n",
    "            if hasattr(self.layers[i], 'setup'):\n",
    "                self.layers[i].setup(optimizer = self.optimizer,loss=self.loss)            \n",
    "\n",
    "    \n",
    "    \"\"\" Iterate through number of epoch on each batch, forward propagate and calculate the loss_gradient from output\n",
    "        and propagates the gradient backwards  \"\"\" \n",
    "    def fit(self, X, y, Xvalid = None, yvalid=None, epochs= 100, batchsize= 64):\n",
    "        \n",
    "        for index in self.progressbar(range(epochs)):\n",
    "            batchloss = []\n",
    "            loss,auc,predictions=None, None, None\n",
    "            for X_batch, y_batch in batchnext(X,y,batchsize = batchsize):\n",
    "                loss,auc,predictions = self._train(X_batch,y_batch)\n",
    "                batchloss.append(loss)\n",
    "            #print(f'Epoch# {index} Training Loss:{loss}  Training Accuracy:{auc}')\n",
    "                \n",
    "                \n",
    "            if (index % 1000 == 0 and Xvalid is not None and yvalid is not None):\n",
    "                mloss = np.mean(batchloss)\n",
    "                val_loss, val_auc,_ = self.predict(Xvalid,yvalid)\n",
    "                print(f'Epoch# {index} Training Loss:{loss} Validation Loss: {val_loss} Training Accuracy:{auc} Validation Accuracy:{val_auc}') \n",
    "            elif(index % 1000 == 0):\n",
    "                print(f'Epoch# {index} Training Loss:{loss}  Training Accuracy:{auc}')\n",
    "                print(predictions)\n",
    "                \n",
    "        \n",
    "    \"\"\"Predicting outcome mainly for validation or test set\"\"\"\n",
    "    def predict(self, X, y):\n",
    "        y_pred = self._forward(X)  \n",
    "        loss = self.loss(y, y_pred)\n",
    "        accuracy = self.loss.auc(y, y_pred)\n",
    "        return loss, accuracy,y_pred\n",
    "    \n",
    "    \"\"\" Training on single batch with gradient updates\"\"\"\n",
    "    def _train(self, X, y):        \n",
    "        y_pred = self._forward(X)        \n",
    "        loss = self.loss(y, y_pred, self._layerweights())\n",
    "        accuracy = self.loss.auc(y,y_pred)\n",
    "        grad = self.loss.gradient(y, y_pred)        \n",
    "        self._backward(grad)\n",
    "        #self._step()\n",
    "        return loss, accuracy,y_pred        \n",
    "        \n",
    "    \n",
    "    #calculate the output by propagating forward\n",
    "    def _forward(self, X, training=True):\n",
    "        layerout= X\n",
    "        for layer in self.layers:\n",
    "            #if(hasattr(layer, 'w')):\n",
    "            #    print(layer.w)\n",
    "            layerout = layer.forward(layerout,training)     \n",
    "            \n",
    "        return layerout\n",
    "    \n",
    "    def _layerweights(self):\n",
    "        layerweigths = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'w'):\n",
    "                layerweigths.append(layer.w)\n",
    "        return layerweigths \n",
    "        \n",
    "    \n",
    "    #Propagate the gradient backwards and update the weights in each layer\n",
    "    def _backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad= layer.backward(grad)\n",
    "            \n",
    "        return grad\n",
    "    \n",
    "    def _step(self):\n",
    "        for layer in reversed(self.layers):\n",
    "            if hasattr(layer, 'step'):\n",
    "                layer.step()\n",
    "            \n",
    "       \n",
    "            \n",
    "    def summary(self , title = \"Model Summary\"):\n",
    "        print (AsciiTable([[title]]).table)\n",
    "        print (\"Input Shape: %s\" % str(self.layers[0].inputshape))\n",
    "        \n",
    "        table_data = [[\"Layer Name\", \"Input Shape\", \"Output Shape\" , \"Shape\"]]\n",
    "        for layer in self.layers:\n",
    "            table_data.append([layer.name, layer.inputshape, layer.outputshape(), layer.shape])\n",
    "            \n",
    "        print (AsciiTable(table_data).table)    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple iterator which yields input as batch based on its batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnext(X,y,batchsize =64):\n",
    "    nSize= X.shape[0]\n",
    "    for b in np.arange(0, nSize, batchsize):\n",
    "        start, end = b , min(nSize, b+batchsize)\n",
    "        yield X[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract/base Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\" returns the name of the layer and mainly for displaying model summary.\"\"\"\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        raise NotImplementedError()       \n",
    "    \n",
    "    def outputshape(self):\n",
    "        raise NotImplementedError()   \n",
    "    \n",
    "    def set_inputshape(self, shape):      \n",
    "        self.inputshape= shape\n",
    "        \n",
    "        \n",
    "    def forward(self, X, training = True):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        raise NotImplementedError()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Layer - Extending the base Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,nunits): \n",
    "        self.nunits = nunits\n",
    "        self.input = None\n",
    "        self.w, self.b = None, None\n",
    "        \n",
    "    def setup(self, optimizer,loss):\n",
    "        self.loss = loss\n",
    "        rangelimit = 1 / math.sqrt(self.inputshape)       \n",
    "        self.w = np.random.uniform(-rangelimit,rangelimit,self.shape)  \n",
    "        self.b = (rangelimit * np.random.random((1,self.nunits))) \n",
    "        self.w_opt = copy.copy(optimizer)\n",
    "        self.b_opt = copy.copy(optimizer)       \n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (self.inputshape ,self.outputshape())\n",
    "    \n",
    " \n",
    "    def outputshape(self):\n",
    "        return self.nunits\n",
    "    \n",
    "    def forward(self, X, training = True):        \n",
    "        self.input = X\n",
    "        return X.dot(self.w) + self.b\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        W = self.w\n",
    "        \n",
    "        self.dw = np.dot(self.input.T,grad)\n",
    "        self.db = np.sum(grad, axis =0, keepdims=True) \n",
    "               \n",
    "        grad = grad.dot(W.T)\n",
    "        \n",
    "        self.w =self.w_opt.update(self.w, self.dw)\n",
    "        self.b = self.b_opt.update(self.b, self.db) \n",
    "        \n",
    "        if hasattr(self.loss, 'reg'):\n",
    "            self.dw += self.loss.reg *  self.w   \n",
    "            \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Class - Simple Extension over Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activationfn):\n",
    "        self.activationfn = activationfn()\n",
    "       \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (self.inputshape ,self.outputshape())\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\" returns the name of the layer and mainly for displaying model summary.\"\"\"\n",
    "        return self.activationfn.__class__.__name__\n",
    "    \n",
    "    \n",
    "    def forward(self, X, training = True):\n",
    "        \n",
    "        self.input = X\n",
    "        self.h = self.activationfn(X)\n",
    "        return self.h\n",
    "        \n",
    "    \n",
    "    def backward(self, grad):        \n",
    "        activationgrad = self.activationfn.gradient(self.h)         \n",
    "        if(activationgrad is None): return grad            \n",
    "        return grad *   activationgrad   \n",
    "    \n",
    "   \n",
    "    def outputshape(self):\n",
    "        return self.inputshape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of Activation functions\n",
    "\n",
    "class sigmoid:\n",
    "    def __call__(self,x):\n",
    "        return 1/(1+np.exp(-x)) \n",
    "      \n",
    "    \n",
    "    def gradient(self,x):\n",
    "        return (x * (1-x))\n",
    "    \n",
    "\n",
    "class relu:\n",
    "    def __call__(self,x):\n",
    "        return x * (x >0)\n",
    "        #return np.where(x>=0, x , 0)\n",
    "    \n",
    "    def gradient(self,x):\n",
    "        return 1. * (x >0)\n",
    "        #return np.where(x>=0, 1 , 0)\n",
    "    \n",
    "    \n",
    "class softmax:\n",
    "    def __call__(self,x):\n",
    "        expo = np.exp(x)\n",
    "        result = expo/np.sum(expo,axis=1, keepdims=True)  \n",
    "        return result\n",
    "    \n",
    "    def gradient(self,x):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A abstract loss class with helps in evaluating the loss value between the predicted and actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "class Loss(object):\n",
    "    def __call__(self, y, p,lweights = None):\n",
    "        pass   \n",
    "\n",
    "        \n",
    "    def auc(self, y, p):\n",
    "        return accuracy_score(y,p)\n",
    "    \n",
    "    def gradient(self, y, p):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    \n",
    "    def __init__(self, reg=1e-3):\n",
    "        self.reg = reg\n",
    "        \n",
    "    def __call__(self, y, p,lweights = None):\n",
    "        return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))    \n",
    "    \n",
    "    def auc(self, y, p):\n",
    "        #print( np.argmax(p, axis=1))\n",
    "        return accuracy_score(y.ravel(), np.argmax(p, axis=1))\n",
    "        \n",
    "   \n",
    "    def gradient(self, y, p):\n",
    "        return y - p \n",
    "    \n",
    "class CrossEntropyForSoftMax(Loss):\n",
    "    \n",
    "    def __init__(self, reg=1e-3):\n",
    "        self.reg = reg\n",
    "    \n",
    "    def __call__(self, y, p,lweights = None):\n",
    "        #select the right propbolity for loss  \n",
    "        correct_prob = -np.log(p[range(len(y)), y.ravel()-1])\n",
    "        dataloss = np.sum(correct_prob)/len(y)       \n",
    "        #regularization can be defined by 1/2 * Reg * np.sum(w*2)\n",
    "        regloss= 0\n",
    "    \n",
    "        if lweights is not None:\n",
    "            for weight in lweights:\n",
    "                regloss +=  0.5* self.reg* np.sum(np.square(weight))\n",
    "        \n",
    "        return dataloss+regloss   \n",
    "    \n",
    "    def auc(self, y, p):\n",
    "        #print( np.argmax(p, axis=1))\n",
    "        return accuracy_score(y.ravel(), np.argmax(p, axis=1)+1)\n",
    "        \n",
    "   \n",
    "    def gradient(self, y, p):\n",
    "        dscore = p\n",
    "        dscore[range(len(y)), y.ravel()-1] -= 1        \n",
    "        dscore /= len(y)\n",
    "        return dscore     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer is a technique which produces gradients iteratively to update weights in order to minimize the cost function or converge to local minima. The traditional Gradient Desent(bacth) requires all the training sampled to be loaded into the memory to calculate the gradients. This is in a real world, seems to be not effective and sometimes not pratical too,  since usually the dataset is bigger in size. To address this issue, instead of loading the Data as a whole, load it in batches and calculuate gradient at batch levelwith same level of accuracy. This is named as SGD - Stochastic Gradient Desent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate = 0.01, momentum=0):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.momentum = momentum\n",
    "        self.w_updated = None\n",
    "    \n",
    "    def update(self, w, grad):\n",
    "        if self.w_updated is None:\n",
    "            self.w_updated = np.zeros(np.shape(w))        \n",
    "        \n",
    "        #use the momentum if any\n",
    "        self.w_updated = (self.momentum * self.w_updated) + (1-self.momentum) * grad        \n",
    "        return w - (self.learning_rate * grad) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat(\"data\\handwritten.mat\")\n",
    "print(data['X'].shape)\n",
    "print(data['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(img, title:None):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    if(title is not None): plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAEICAYAAACj9mr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwsySHBBmluSAMLMkB4SZJf0/ZE4Quqx+he0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fec835bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reshape by 20*20\n",
    "image = np.reshape(data['X'][4000],(20,20))\n",
    "image.shape\n",
    "show(image, data['y'][4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y =  data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 400)\n",
      "(1000, 400)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X,y, test_size=0.20, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(learning_rate =  0.1,momentum=0.9)\n",
    "loss = CrossEntropyForSoftMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([    \n",
    "    Dense(100),\n",
    "    Activation(relu),    \n",
    "    Dense(10),\n",
    "    Activation(softmax)    \n",
    "],  optimizer, loss, X.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (4 of 10000) |                       | Elapsed Time: 0:00:00 ETA:  0:10:39"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0 Training Loss:2.3403229024928573 Validation Loss: 2.3074124730034136 Training Accuracy:0.0945 Validation Accuracy:0.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1005 of 10000) |##                  | Elapsed Time: 0:00:51 ETA:  0:07:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1000 Training Loss:0.265501929056545 Validation Loss: 0.33293805906750074 Training Accuracy:0.94525 Validation Accuracy:0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% (2004 of 10000) |####                | Elapsed Time: 0:01:45 ETA:  0:06:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 2000 Training Loss:0.2077465428677755 Validation Loss: 0.307015676711618 Training Accuracy:0.97025 Validation Accuracy:0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% (3003 of 10000) |######              | Elapsed Time: 0:02:30 ETA:  0:05:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 3000 Training Loss:0.18037721176037894 Validation Loss: 0.2957282513857925 Training Accuracy:0.981 Validation Accuracy:0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% (4004 of 10000) |########            | Elapsed Time: 0:03:14 ETA:  0:04:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 4000 Training Loss:0.16830148054695862 Validation Loss: 0.29458238505657175 Training Accuracy:0.99025 Validation Accuracy:0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% (5004 of 10000) |##########          | Elapsed Time: 0:04:07 ETA:  0:04:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 5000 Training Loss:0.1654000325474913 Validation Loss: 0.29805013375393397 Training Accuracy:0.99575 Validation Accuracy:0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% (6005 of 10000) |############        | Elapsed Time: 0:04:59 ETA:  0:03:22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 6000 Training Loss:0.16722798929517077 Validation Loss: 0.30464483799744313 Training Accuracy:0.99825 Validation Accuracy:0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% (7004 of 10000) |##############      | Elapsed Time: 0:05:47 ETA:  0:02:24"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 7000 Training Loss:0.17126566155588857 Validation Loss: 0.31207865133132734 Training Accuracy:0.9995 Validation Accuracy:0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% (8003 of 10000) |################    | Elapsed Time: 0:06:37 ETA:  0:01:39"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 8000 Training Loss:0.176136726107794 Validation Loss: 0.31906940997525635 Training Accuracy:0.99975 Validation Accuracy:0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% (9004 of 10000) |##################  | Elapsed Time: 0:07:25 ETA:  0:00:47"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 9000 Training Loss:0.18116820783995924 Validation Loss: 0.3254068859103843 Training Accuracy:1.0 Validation Accuracy:0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10000 of 10000) |###################| Elapsed Time: 0:08:14 Time: 0:08:14\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train,X_valid,y_valid,epochs= 10000,batchsize= 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| Model Summary |\n",
      "+---------------+\n",
      "Input Shape: 400\n",
      "+------------+-------------+--------------+------------+\n",
      "| Layer Name | Input Shape | Output Shape | Shape      |\n",
      "+------------+-------------+--------------+------------+\n",
      "| Dense      | 400         | 100          | (400, 100) |\n",
      "| relu       | 100         | 100          | (100, 100) |\n",
      "| Dense      | 100         | 10           | (100, 10)  |\n",
      "| softmax    | 10          | 10           | (10, 10)   |\n",
      "+------------+-------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
