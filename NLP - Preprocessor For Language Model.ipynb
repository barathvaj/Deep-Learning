{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP - Preprocessor For Language Model\n",
    "\n",
    "Our objective here is to Create a language model from scratch. The language model either can be at character or word level. Given the group of words or character predict the forthcoming character or a word.\n",
    "\n",
    "There is a excellent explanation given here for RNN and LSTM. Please check it below\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n",
    "\n",
    "In this session, let us take the csv as an input and outputs the numerical data which would be used with the neural net to build a language model or for sentiment classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LanguageTokenizer is a wrapper on top of the spacy tokenizer which helps in preprocessing the text before giving to tokenization process to Spacy. \n",
    "\n",
    "The preprocess steps includes \n",
    "\n",
    "1) Replace the repeated characters with the t_rep tag. for example 22222 will be replaced as t_rep 2 5\n",
    "\n",
    "2) Replace the repeated words with the t_wrep tag. for example 'Test Test Test Test' will be replaced as t_wrep test 4\n",
    "\n",
    "3) Replace the Caps word with the tag . for example 'HOW are you?' will be replaced with 't_up how are you?'\n",
    "\n",
    "4) Replace Html BR tags with /n\n",
    "\n",
    "5) Replace Multi spaces to a single spaces\n",
    "\n",
    "Thanks to the fast ai course which helps in deriving these common initutions \n",
    "\n",
    "Post this preprocessed text will be tokenized using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import re \n",
    "\n",
    "class LanguageTokenizer:\n",
    "    def __init__(self,language='en'):\n",
    "        self.processor = spacy.load('en')   \n",
    "        for w in ('_eos_','_bos_','_unk_'):\n",
    "            self.processor.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "            \n",
    "        self.re_repeatedCharacters = re.compile(r'(\\S)(\\1{3,})')\n",
    "        self.re_repeatedWords = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "        self.re_breaks = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)  \n",
    " \n",
    "    def substituteBreaks(self,text):           \n",
    "        return self.re_breaks.sub(\"\\n\", text)   \n",
    "    \n",
    "    def _substituteRepeatedCharacters(self,r):\n",
    "        TK_REP=  'tk_rep'      \n",
    "        c,cc = r.groups()        \n",
    "        return f' {TK_REP} {len(cc)+1} {c} '   \n",
    "   \n",
    "    def _substituteRepeatedWords(self,r):\n",
    "        TK_WREP = 'tk_wrep'\n",
    "        c,cc = r.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "        \n",
    "    def substituteCaps(self,text):\n",
    "        TOK_UP,TOK_SENT,TOK_MIX = ' t_up ',' t_st ',' t_mx '\n",
    "        res = []\n",
    "        prev='.'\n",
    "        re_word = re.compile('\\w')\n",
    "        re_nonsp = re.compile('\\S')\n",
    "        for s in re.findall(r'\\w+|\\W+', text):\n",
    "            res += ([TOK_UP,s.lower()] if (s.isupper() and (len(s)>2))   \n",
    "                    else [s.lower()]) \n",
    "        return ''.join(res)\n",
    "    \n",
    "    #subtitute multi spaces with single space        \n",
    "    def substituteSpaces(self,text):\n",
    "        return re.sub(' {2,}', ' ', text)\n",
    "    \n",
    "    \n",
    "    def processText(self, text):\n",
    "        text = self.re_repeatedCharacters.sub(self._substituteRepeatedCharacters, text)\n",
    "        text = self.re_repeatedWords.sub(self._substituteRepeatedWords, text)\n",
    "        text = self.substituteCaps(text)\n",
    "        text = self.substituteBreaks(text)\n",
    "        text = self.substituteSpaces(text)\n",
    "        \n",
    "        return [token.text for token in self.processor.tokenizer(text)]\n",
    "        \n",
    "    \n",
    "    def process(self, texts):        \n",
    "        return [self.processText(text) for text in texts]        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LanguagePreprocessor is the wrapper which accepts the CSV and it should have a column names 'text and labels' and returns the numerical representation for both training set and validation set. \n",
    "\n",
    "1) It creates the vocabalary dictionary using the max_vocab_size and min_word_freq as a hyper-parameter\n",
    "2) It returns numercalize input based on the vocabalary dictionary and this in turn can be used for further processing for language model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import pickle\n",
    "import ipdb as pdb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "\n",
    "#Always assumes the CSV has text and labels as columns\n",
    "class LanguagePreprocessor:\n",
    "    def __init__(self,csvPath,chunkSize = 50000,validationSetSize = 0.1,labelReplace=None):\n",
    "        self.tokenizer = LanguageTokenizer('en') \n",
    "        self.trn_texts, self.trn_labels, self.val_texts, self.val_labels =self._splitDataset(csvPath, validationSetSize,chunkSize,labelReplace)\n",
    "        \n",
    "    def apply(self, max_vocab_size = 60000, min_word_freq = 2):       \n",
    "        self.trn_tokens= self.tokenizer.process(self.trn_texts)\n",
    "        self.val_tokens= self.tokenizer.process(self.val_texts)      \n",
    "        \n",
    "        self._buildVocabDict(max_vocab_size,min_word_freq)\n",
    "        return self._numericalizeByVocabDict()        \n",
    "        \n",
    "    def _getTokens(self):\n",
    "        return self.tokenizer.process(trn_texts)\n",
    "    \n",
    "    def _buildVocabDict(self, max_vocab_size = 60000, min_word_freq = 2, bos_token = \"_bos_\", unk_token=\"_unk_\", eos_token = \"_eos_\"):\n",
    "        freq = Counter(p for o in self.trn_tokens for p in o)\n",
    "        #pdb.set_trace()\n",
    "        self.numbertotoken=  [o for  o,c in freq.most_common(max_vocab_size) if c> min_word_freq]\n",
    "        self.numbertotoken.insert(0, bos_token)       \n",
    "        self.numbertotoken.insert(0, eos_token)\n",
    "        self.numbertotoken.insert(0, unk_token)        \n",
    "        self.tokentonumber = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(self.numbertotoken)}) \n",
    "        \n",
    "        return self.numbertotoken, self.tokentonumber\n",
    "    \n",
    "    def _numericalizeByVocabDict(self):\n",
    "        self.trn_lm = np.array([[self.tokentonumber[o] for o in p] for p in self.trn_tokens])\n",
    "        self.val_lm = np.array([[self.tokentonumber[o] for o in p] for p in self.val_tokens])\n",
    "        return self.trn_lm, self.val_lm\n",
    "        \n",
    "        \n",
    "    def _fixup(self,x):\n",
    "        re1 = re.compile(r'  +')\n",
    "        x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "        return re1.sub(' ', html.unescape(x))    \n",
    "        \n",
    "    def _parsedInput(self ,df,labelReplace,ini_texts,ini_labels):\n",
    "        BOS = '_bos_'  # beginning-of-sentence tag\n",
    "        if(labelReplace is not None):\n",
    "             df.replace({'labels': {'neg': 0, 'pos': 1, 'unsup':2}},inplace=True)\n",
    "        \n",
    "        labels = df.iloc[:,-1].astype(np.int64)\n",
    "        texts = f'\\n{BOS} ' + df[\"text\"].astype(str) \n",
    "        \n",
    "        ini_texts =   ini_texts + list(texts.apply(self._fixup).values) if ini_texts is not None else list(texts.apply(self._fixup).values)\n",
    "        ini_labels = ini_labels + labels.tolist() if ini_labels is not None else labels.tolist()  \n",
    "        \n",
    "        return ini_texts, ini_labels\n",
    "        \n",
    "    def _splitDataset(self,csvPath, validationSetSize,chunksize,labelReplace):\n",
    "       \n",
    "        trn_txt_df, trn_label_df, val_txt_df,val_label_df =[],[],[],[]        \n",
    "        df =  pd.read_csv(csvPath, chunksize=chunksize) if chunksize is not None else pd.read_csv(csvPath) \n",
    "            \n",
    "        trn_texts, trn_labels, val_texts, val_labels = None,None,None,None\n",
    "        \n",
    "        for i,data in enumerate(df):\n",
    "            trn_data,val_data= train_test_split(\n",
    "                data, test_size=validationSetSize)\n",
    "            \n",
    "            trn_texts, trn_labels = self._parsedInput(trn_data , labelReplace,trn_texts, trn_labels)\n",
    "            val_texts, val_labels= self._parsedInput(val_data , labelReplace,val_texts, val_labels)\n",
    "            \n",
    "            \n",
    "        return trn_texts, trn_labels, val_texts, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3744: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  regex=regex)\n"
     ]
    }
   ],
   "source": [
    "preprocessor = LanguagePreprocessor('data/imdb_master.csv',labelReplace={'labels': {'neg': 0, 'pos': 1, 'unsup':2}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ [40, 41, 12, 14, 16, 9947, 5653, 1976, 705, 454, 29, 313, 717, 5, 6365, 31, 1533, 1579, 28, 5, 7, 7018, 566, 6, 563, 18, 327, 710, 140, 2894, 5, 500, 7355, 45, 3, 6438, 79, 8, 7, 208, 16063, 6, 36, 15359, 399, 31, 7075, 10933, 28, 4, 66, 12145, 1172, 8605, 58, 14, 412, 17, 31, 79, 28, 5, 216, 5, 6365, 686, 333, 1470, 101, 7, 37801, 770, 15, 6262, 7, 2424, 0, 56, 16, 1962, 9, 26, 17, 4021, 4206, 4, 20, 27, 81, 151, 32, 554, 891, 148, 805, 5, 6, 123, 3, 26, 32, 163, 90, 20, 3, 16, 106, 85, 35, 1639, 36, 3859, 56, 16, 289, 5, 23, 1625, 4, 10933, 3676, 55, 180, 134, 5, 151, 38, 7019, 139, 5, 6, 1457, 9, 16, 5726, 154, 16, 1329, 4, 11, 17, 13088, 4, 3, 313, 958, 8, 55, 1740, 6, 1883, 27532, 151, 38, 601, 5, 45, 3, 73, 495, 4, 1579, 1659, 9, 180, 796, 4, 873, 5, 55, 15873, 93, 38, 12, 890, 18, 25, 7, 3449, 93, 37, 38, 110, 763, 12, 374, 3, 16, 5588, 16, 53, 16, 395, 1954, 16, 154, 4, 3, 26, 71, 4095, 49, 7, 848, 18, 824, 18, 106, 58, 7, 16, 167, 17, 610, 16, 56, 20, 1579, 6, 3287, 33276, 31, 21, 6943, 28, 57, 30, 326, 5, 2595, 5, 840, 21, 7, 382, 4, 7, 147, 53, 124, 7285, 78, 639, 250, 37, 1686, 288, 3, 894, 4, 886, 5, 3, 193, 10, 14475, 4, 1579, 1549, 46, 4815, 600, 3538, 5, 6, 33, 7, 16, 792, 10456, 16, 4, 4161, 450, 31, 21, 2156, 28, 5, 910, 8284, 31, 21, 0, 28, 5, 6, 9419, 14193, 31, 5068, 0, 28, 34, 42, 429, 4, 20, 135, 8, 42, 5, 10933, 17, 16, 5609, 16, 3072, 12, 3, 1028, 2299, 8, 2040, 4, 1625, 4, 450, 10, 2271, 12, 663, 10933, 250, 38, 5, 16, 27, 137, 5, 51, 15, 227, 49, 61, 2140, 61, 4, 16, 52, 37, 96, 455, 169, 66, 13451, 745, 133, 2484, 480, 9, 2058, 22, 367, 5, 708, 9, 7, 37802, 0, 8, 591, 130, 445, 5, 44, 93, 37, 1923, 10933, 23, 64, 1439, 5884, 4, 69, 10, 15, 65, 4, 6, 5, 167, 2045, 39732, 421, 7836, 1438, 18, 35, 74, 179, 402, 7, 10613, 49, 7, 32089, 1600, 2489, 4, 20, 452, 5, 941, 39732, 5, 34631, 36134, 5, 6, 3489, 2117, 1786, 13577, 49, 2647, 23932, 2589, 4, 660, 203, 911, 190, 3, 327, 16063, 31, 0, 28, 2045, 39732, 5898, 1533, 1579, 5, 7075, 10933, 5, 4161, 450, 5, 9419, 14193],\n",
       "        [40, 41, 13, 581, 9, 1307, 14, 24, 29, 563, 327, 261, 4, 13, 231, 3, 647, 916, 6, 215, 14, 306, 65, 4, 87, 13, 19, 68, 379, 4, 150, 33, 7864, 95, 31, 11, 83, 64, 170, 6, 4515, 0, 3511, 118, 292, 8, 16841, 4, 20, 11, 685, 139, 87, 212, 23, 5727, 0, 277, 7, 24932, 3652, 8, 2713, 66, 55, 593, 897, 55, 6, 55, 1241, 6210, 10, 1878, 4, 25, 5727, 59, 273, 148, 20564, 367, 50, 87, 177, 18, 120, 8481, 1390, 8606, 4, 3, 458, 19, 30, 5727, 5, 11, 19, 3, 4805, 898, 15, 12544, 42, 138, 3, 286, 4, 11, 19, 21, 62, 3, 941, 222, 37, 96, 552, 63, 103, 88, 57, 381, 4, 3, 244, 102, 467, 68, 2038, 4, 20, 64, 415, 47, 37, 5292, 805, 29, 14, 24, 37, 3749, 152, 11, 85, 30, 3939, 7, 1347, 778, 4, 3, 300, 310, 13, 120, 71, 37, 96, 9, 789, 4506, 6, 3, 1502, 71, 37, 1878, 11, 4, 20, 277, 324, 42, 15, 13, 215, 3, 413, 266, 47, 273, 0, 19, 836, 6, 3534, 3, 24, 5, 33, 9, 752, 106, 337, 69, 10, 4],\n",
       "        [40, 41, 63, 46, 3702, 26, 4, 57, 30, 97, 86, 379, 5, 2208, 6244, 6, 6263, 2698, 34, 65, 168, 5, 25, 1193, 49, 7, 187, 232, 288, 1294, 5, 3, 26, 10, 678, 596, 70, 23, 3355, 39733, 5456, 5, 2501, 5, 16245, 6, 1263, 6524, 60, 66, 253, 284, 379, 4, 20, 211, 15, 23, 4265, 15508, 9, 111, 270, 3, 328, 149, 406, 63, 17, 178, 29, 31, 3, 147, 131, 6263, 2698, 2652, 9, 7, 1606, 26141, 106, 35, 222, 38, 51, 14081, 12, 340, 2376, 10, 176, 67, 91, 7, 1006, 28, 6, 27, 37, 7, 10197, 198, 705, 33, 1351, 8, 3, 133, 4, 20, 303, 202, 54, 440, 15, 3, 161, 2485, 202, 3323, 10, 33, 12, 3, 932, 8, 158, 7, 244, 48, 20],\n",
       "        ...,\n",
       "        [40, 41, 13, 37, 9, 446, 43, 42, 3, 805, 29, 14, 1819, 77, 153, 14, 24, 17, 127, 53, 3, 119, 34, 33, 16, 4148, 16, 4, 20, 13, 436, 757, 3, 507, 14, 24, 19, 342, 12, 5, 31, 13, 19, 46, 1567, 22, 39, 269, 5, 6, 7, 16, 772, 12, 16, 22, 124, 501, 29, 14, 26, 4, 11, 19, 3295, 48, 28, 6, 280, 86, 5, 3, 119, 34, 33, 80, 840, 5, 27, 76, 922, 2458, 8, 109, 12, 413, 7379, 42, 160, 3, 1236, 4, 20, 52, 10, 7, 217, 1546, 220, 515, 6, 853, 658, 134, 4, 291, 98, 12, 68, 2524, 3752, 6, 2953, 2447, 9, 842, 15, 4, 200, 1232, 5, 13, 137, 455, 98, 198, 146, 12, 3, 0, 47, 111, 3, 4240, 8, 16, 9151, 16, 181, 2736, 3050, 4, 20, 15, 2812, 342, 198, 5, 13, 54, 37, 9, 153, 11, 17, 7, 81, 1111, 26, 4, 11, 59, 7, 184, 8, 877, 6, 64, 65, 121, 979, 5, 74, 12, 3, 1299, 591, 4, 11, 2195, 413, 5, 413, 507, 799, 204, 5300, 6, 23, 7, 100, 829, 8, 1443, 4],\n",
       "        [40, 41, 14, 10, 39, 8, 3, 267, 116, 8, 42, 73, 4, 3, 674, 18, 3039, 18, 6, 18, 155, 10386, 3849, 47, 8865, 14, 7, 307, 151, 38, 9315, 4469, 49, 8438, 4, 20, 842, 15, 11, 19, 110, 29, 7, 6187, 18, 3237, 354, 5, 6, 15, 43, 39, 241, 3309, 1780, 3, 267, 2201, 2418, 12, 24, 499, 130, 842, 15, 35, 325, 36, 3595, 139, 12, 185, 99, 147, 5, 425, 23, 1746, 3495, 130, 842, 15, 3, 11336, 32860, 154, 34, 551, 342, 12, 180, 796, 130, 842, 74, 15, 3, 133, 10, 8, 158, 9185, 11217, 15, 625, 3106, 31, 39, 8, 3, 267, 168, 12, 16, 1013, 8, 3, 588, 355, 16, 28, 10, 245, 6, 260, 3, 2026, 12961, 12, 1784, 4, 3, 210, 15, 1782, 7389, 21, 92, 21, 7, 20001, 12, 7, 244, 15, 39, 3197, 765, 22, 39, 564, 93, 734, 37, 243, 142, 10, 63, 81, 3343, 75, 351, 4, 3, 850, 10, 2310, 389, 5, 6, 3, 2446, 8, 11, 10, 21, 1245, 18, 2642, 6, 2482, 21, 253, 27, 196, 1314, 9, 82, 4, 108, 1485, 116, 34, 1052, 23, 67, 5884, 22, 127, 4, 20, 13, 407, 16, 5918, 12, 180, 796, 16, 9, 268, 47, 1194, 9, 156, 9, 3, 5462, 53, 4214, 7, 264, 2852, 12, 3, 3421, 4, 14, 10, 46, 564, 6, 7, 331, 8, 1055, 7162, 4],\n",
       "        [40, 41, 14, 16, 24, 16, 10, 54, 32, 396, 48, 2034, 51, 14, 151, 38, 4999, 5, 105, 158, 116, 34, 54, 110, 5, 105, 3, 228, 26, 19, 7, 2675, 4, 13, 740, 116, 15, 2294, 3, 1296, 8, 3, 104, 26, 4, 6959, 287, 10, 172, 65, 516, 22, 15, 4, 95, 885, 5, 662, 168, 6, 79, 31, 12401, 12, 3, 6000, 2298, 90, 28, 4, 9, 3, 666, 2221, 94, 131, 3, 228, 85, 7, 100, 317, 1840, 46, 200, 2913, 877, 531, 4, 14, 0, 1534, 18, 139, 467, 67, 51, 46, 13533, 70, 350, 8, 15294, 53, 14, 1572, 505, 180, 2546, 8, 1533, 2613, 224, 77, 151, 102, 38, 4999, 4]], dtype=object),\n",
       " array([ [40, 41, 3, 104, 955, 19, 100, 18, 65, 1506, 8, 3, 317, 6, 3, 616, 6, 377, 43, 363, 4, 11, 19, 174, 7, 204, 170, 140, 4, 20, 163, 11, 296, 70, 189, 6, 3, 616, 6, 3, 124, 888, 5033, 34, 777, 186, 6, 3, 237, 166, 3002, 205, 3, 13765, 1060, 4, 74, 3, 173, 143, 47, 1157, 3, 1060, 59, 777, 4305, 18, 114, 115, 138, 3, 364, 5, 131, 177, 11, 19, 429, 4, 20, 15, 6, 72, 67, 2503, 4260, 18, 39, 8, 3, 135, 276, 550, 12, 359, 4, 20, 14, 10, 63, 570, 66, 27, 303, 64, 12783, 97, 78, 932, 29, 7, 140, 4, 27, 76, 238, 82, 3, 2074, 16, 3, 173, 143, 10, 170, 5, 1525, 1147, 29, 107, 5, 111, 107, 114, 138, 3, 364, 6, 111, 11, 42, 58, 3, 13765, 393, 11, 258, 38, 7, 10047, 350, 8, 3, 1058, 48, 16, 6950, 5, 72, 888, 5033, 5, 72, 123, 4],\n",
       "        [40, 41, 14, 26, 10, 63, 108, 8, 3, 1501, 59, 1509, 106, 9, 111, 94, 32, 251, 732, 5, 1180, 155, 77, 10, 756, 9, 98, 8, 42, 2221, 4, 14, 895, 3685, 195, 11538, 76, 38, 527, 45, 312, 49, 18128, 47, 51, 3, 2808, 6, 1031, 21168, 9, 78, 9255, 47, 406, 3, 1213, 1966, 4, 20, 14, 26, 59, 468, 21, 65, 21, 118, 13, 157, 141, 128, 5, 6, 13, 403, 15, 1229, 4, 11, 10, 102, 2710, 23, 2296, 1724, 8, 471, 5, 6, 271, 147, 59, 50, 125, 1985, 1366, 12, 3, 943, 15, 13, 164, 178, 9, 37, 9, 807, 11, 5, 62, 80, 12, 654, 9, 370, 42, 3, 3694, 6, 0, 12, 4056, 195, 3122, 17, 192, 4, 20, 13, 1286, 9, 82, 14, 24, 43, 239, 1556, 67, 12, 2218, 5, 113, 807, 11, 29, 32, 285, 4, 27, 115, 151, 82, 11, 4],\n",
       "        [40, 41, 95, 5, 7, 184, 7, 630, 4, 11, 5888, 12778, 5, 102, 7, 95, 24, 48, 109, 11129, 66, 16, 8115, 711, 16, 6361, 19, 878, 4, 15, 2669, 154, 110, 11, 462, 4, 6, 11722, 1309, 7, 16, 25804, 16, 48, 15, 594, 1057, 389, 48, 13, 218, 3422, 430, 48, 105, 44, 5888, 5, 3, 127, 19, 389, 5, 32, 3, 32, 237, 32, 24, 32, 19, 32, 3949, 32, 198, 32, 42, 32, 3, 32, 114, 48],\n",
       "        ...,\n",
       "        [40, 41, 13, 316, 14, 24, 66, 13, 19, 208, 31, 205, 927, 28, 6, 1574, 11, 1869, 229, 66, 178, 9, 3, 395, 1056, 4, 163, 5, 58, 3111, 169, 329, 13, 149, 132, 11, 6, 254, 276, 946, 9, 11, 572, 60, 29, 32, 285, 50, 13, 76, 219, 11, 4, 33, 270, 768, 11, 19, 3, 16208, 1261, 5, 53, 3, 210, 15, 69, 83, 7, 2658, 22, 7, 135, 442, 5, 25, 11, 19, 7, 3894, 1708, 543, 4],\n",
       "        [40, 41, 14, 0, 445, 7594, 7, 65, 890, 94, 63, 570, 66, 482, 30299, 23, 482, 5, 21, 11224, 333, 1422, 11, 4, 13, 1621, 27, 15, 13, 19, 453, 8, 2562, 45, 64, 32392, 0, 3, 6782, 3, 8841, 84, 643, 1613, 84, 379, 22, 3, 827, 6, 3, 0, 19, 46, 31, 551, 28, 293, 39, 4, 216, 5, 3, 373, 8, 3, 413, 3585, 524, 6, 3, 11652, 18, 45257, 1424, 18, 9914, 84, 68, 65, 2532, 4, 20, 3760, 26092, 19, 7, 143, 485, 4072, 9, 36, 3629, 5, 149, 4353, 133, 378, 3, 145, 8, 36, 68, 213, 134, 5, 6, 13, 19, 5368, 45, 3, 210, 15, 0, 19, 333, 7, 1054, 7387, 1248, 12888, 48, 20, 42, 12, 42, 5, 14, 10, 39, 503, 438, 5, 1195, 3921, 9, 3, 133, 15, 10, 29, 2326, 146, 4],\n",
       "        [40, 41, 827, 122, 5, 66, 87, 110, 34, 7, 1102, 1795, 4, 225, 49, 3, 1194, 8, 30977, 18, 1846, 18, 36024, 2131, 3, 1191, 16062, 9888, 5, 148, 1430, 37, 3, 641, 9, 7509, 39, 159, 12, 73, 9, 11780, 162, 2221, 4, 63, 250, 0, 125, 802, 8, 14, 504, 10, 3, 4990, 3767, 6388, 343, 45, 64, 24, 1203, 9, 13527, 426, 220, 210, 6, 1100, 4, 2159, 5, 13, 71, 149, 153, 15, 34555, 9888, 10, 7, 100, 798, 6, 7, 87, 110, 26, 48, 20, 454, 29, 3, 1095, 12152, 5, 3, 79, 10, 58, 208, 30977, 4449, 37754, 27332, 31, 14080, 28, 402, 38289, 9, 3, 2623, 8, 0, 34555, 31, 11369, 28, 5, 7, 1127, 8292, 22, 1007, 3137, 4, 25, 5, 136, 57, 214, 137, 15, 14, 1403, 71, 5276, 139, 7, 8900, 893, 12, 3, 3581, 8, 208, 35576, 0, 101, 46, 42, 929, 4449, 9888, 35374, 13399, 49, 251, 5, 1720, 1679, 195, 35131, 1210, 4, 20, 21, 158, 5, 3, 79, 1075, 59, 7, 475, 913, 46, 145, 15, 118, 843, 15136, 71, 181, 22, 4, 12, 210, 5, 52, 10, 7, 136, 242, 8, 294, 4, 27, 76, 182, 893, 5, 209, 598, 214, 423, 6, 1687, 22274, 5, 1038, 2341, 1604, 5, 787, 5, 100, 276, 472, 2092, 5, 6, 108, 3433, 5, 3, 32545, 6, 18569, 8, 0, 6154, 4, 25, 5, 11, 0, 7188, 21, 7, 637, 374, 5, 249, 3, 1120, 8, 34555, 12, 0, 134, 10, 12, 409, 9993, 4, 20, 3, 2607, 216, 5, 10, 39, 143, 47, 1431, 60, 3, 108, 12, 3, 24, 5, 14080, 16837, 47, 59, 299, 189, 2199, 36, 340, 9823, 4, 36, 640, 6, 264, 175, 10, 4702, 12, 185, 2016, 15, 35, 10, 12, 5, 38, 11, 17251, 34555, 53, 29, 3, 7469, 4, 27, 71, 81, 280, 107, 9, 38, 0, 18, 1846, 18, 0, 31, 4449, 8, 2420, 28, 4, 36, 19393, 147, 23, 3, 2525, 6, 3, 20127, 4654, 220, 3, 472, 2092, 34, 576, 26585, 4, 20, 3, 14080, 18, 6966, 1221, 128, 12, 22986, 287, 59, 33, 54, 96, 3843, 146, 25, 59, 596, 9, 7, 282, 644, 3450, 4, 11369, 110, 7, 100, 1103, 9, 320, 7, 2432, 6, 15348, 2623, 4, 3, 1221, 6, 3, 3767, 1201, 76, 38, 128, 12, 154, 131, 9888, 18291, 36, 5566, 9, 370, 6, 944, 5, 36, 2990, 66, 2790, 0, 5, 3, 20127, 6, 154, 66, 3, 382, 37, 78, 2020, 401, 4, 20, 23, 7, 297, 6, 7, 331, 564, 10347, 6, 748, 8, 1374, 18, 1622, 5, 34555, 9888, 43, 229, 5, 206, 3430, 4, 7, 11765, 244, 11252, 139, 43, 239, 331, 564, 49, 3, 26, 93, 37, 1686, 4, 3, 787, 5, 0, 18, 1846, 18, 0, 5, 0, 18, 1445, 18, 0, 0, 6, 3, 42781, 41945, 2841, 41945, 772, 60, 4, 62, 33, 7, 2675, 5, 3, 24, 103, 421, 156, 198, 12, 499, 21, 7, 87, 110, 24, 4, 1174, 22, 7, 68, 2916, 328, 80, 48]], dtype=object))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.apply(max_vocab_size=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the preprocess file for the next session(building language model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill \n",
    "\n",
    "with open('data/imdbpreprocesseddata.file', \"wb\") as dill_file:\n",
    "    dill.dump(preprocessor, dill_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
