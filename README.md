# Deep-Learning Coding Challenge

Objective here is to explore the concepts of deep learning and implementing it from scratch in a structured way. We will test the network with our default handwritten dataset.


## Session 1 : Building Layer based API for builiding and extending neural network
 
**About** : In this part of this exercise, we will create a Layer and Sequential API for building neural network at ease. We will also implement SGD with momentum and popular activation functions


**Link of Work:**   [Code](https://github.com/barathvaj/Deep-Learning/blob/master/Build%20Layer%20Based%20API%20-%20Neural%20Network%20with%20SGD.ipynb)


## Session 2 - Types of Optimizers and its implementation

**About** : In this exercise, we will try to explore different optimizers, extend those and implement it as part of the Layered API and we also test those with our default dataset

**Link of Work:**   [Code](https://github.com/barathvaj/Deep-Learning/blob/master/Types%20of%20Optimizers%20and%20its%20implementation.ipynb)


## Session 3 -  DropOut 

**About** : In this session, we will discuss dropout in detail and compare it with ensemble or bagging techniques. Extend Dropout as part of Layered API.

**Link of Work:** [Code](https://github.com/barathvaj/Deep-Learning/blob/master/Dropout.ipynb)


## Session 4 -  Batch Normalization 

**About** : In this session, we will extend our layer API to support batch normalization.

**Link of Work:** [Code](https://github.com/barathvaj/Deep-Learning/blob/master/BatchNormalization.ipynb)

## Session 4 -  Language Model Preprocessor

**About** : In this session, we will create a wrapper which accepts the input as a csv and return back numercalized data which would be used for building a language model in upcoming sessions.

**Link of Work:** [Code](https://github.com/barathvaj/Deep-Learning/blob/master/NLP%20-%20Preprocessor%20For%20Language%20Model.ipynb)

